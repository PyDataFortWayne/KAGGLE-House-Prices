{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns # heatmaps yay\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_missing_features(df):\n",
    "    tot_rec = len(df.index)\n",
    "    for col in df.columns.values:\n",
    "        if df[col].isnull().sum() / tot_rec > 0.15:\n",
    "            del df[col]\n",
    "\n",
    "def process_df(df):\n",
    "    df['CentralAir'] = df['CentralAir'].map({'Y':1,'N':0}).astype(int)\n",
    "    \n",
    "    drop_high_missing_features(df)\n",
    "    mask = df['Electrical'].notna()\n",
    "    df = df.loc[(mask), :]\n",
    "    #df.drop(df['Electrical'].isnull(), axis=0)\n",
    "    \n",
    "    # Drop TotRmsAbvGrd - Same type of information as GrLivArea\n",
    "    del df['TotRmsAbvGrd']\n",
    "    # Same for GarageCars (GarageArea)\n",
    "    \n",
    "    \n",
    "    for c in ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars',\n",
    "              'GarageQual', 'GarageCond', 'BsmtExposure', 'BsmtFinType2', 'BsmtUnfSF',\n",
    "              'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'BsmtFinSF1', 'BsmtFinSF2', \n",
    "              'MasVnrArea', 'MasVnrType']:\n",
    "        if c in df.columns.values:\n",
    "            del df[c]\n",
    "        \n",
    "    df['GrLivArea'] = np.log(df['GrLivArea']) \n",
    "\n",
    "    if 'SalePrice' in df.columns.values:\n",
    "        df['SalePrice'] = np.log(df['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the train csv file to take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('data','train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records are we dealing with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine which features are important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of each feature is null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls = df.copy().isnull().sum()\n",
    "df_nulls = df_nulls.to_frame().rename(columns={0:'num_nulls'})\n",
    "df_nulls['total_records'] = len(df.index)\n",
    "df_nulls['pct_null'] = df_nulls['num_nulls'] / df_nulls['total_records'] * 100\n",
    "df_nulls[df_nulls['num_nulls'] > 0].sort_values(by='pct_null', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several features which have a high null rate. If we use 15% as a cutoff to say we don't want to use these features we can just drop PoolQC, MiscFeature, Alley, Fence, FireplaceQu and LotFrontage. This is taken care in the `drop_high_missing_features` function written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_high_missing_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also drop the one record where Electical is null. This won't adversely affect the overall model since it's a single record. This will be done in the `process_df` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting note is that the GarageX type features all have the same amount of null values. This is likely because they're a part of the same records. We can verify this to see how many records have all those features set to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['GarageType'].isnull() &\n",
    "       df['GarageYrBlt'].isnull() &\n",
    "       df['GarageFinish'].isnull() &\n",
    "       df['GarageQual'].isnull() &\n",
    "       df['GarageCond'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's confirmed. All 81 are a part of the same record. We won't need to worry about these though as they are being dropped since they are reduntant (for example GarageYrBlt is nearly the same as YrBuilt or GarageQual is a similar feature to OverallQual). A case could be made to keep these features but for now they will be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's take a look at the correlations between each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (30,20))\n",
    "sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True);\n",
    "plt.title(\"Heatmap of Feature Correlation\", fontsize = 30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the correlation heat map we can quickly identify features which are highly related to other features. For example 1stFlSF and TotalBsmntSF are highly correlated which makes sense because generally speaking most basements are full basements (the basement is wearing the first floor like a hat). The same goes for 1stFlSF and 2ndFlSF (This house is a mad hatter).\n",
    "\n",
    "Another instance is GarageCars and GarageArea. This makes sense because you need more space to store more cars. This is the same rationale for TotRmsAbvGrd (Total rooms above ground) and GrLivArea (ground floor living area).\n",
    "\n",
    "YearBuilt and GarageYrBlt also appear to be highly correlated. This makes sense because typically the house and the garage are built at the same time.\n",
    "\n",
    "Let's take some time to graph some of these relationships and see what comes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (20,12))\n",
    "sns.scatterplot(x=df['GrLivArea'], y= df['SalePrice']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = (df['GrLivArea'] > 4000) & (df['SalePrice'] < 200000)\n",
    "df[filter][['GrLivArea', 'SalePrice']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graph there are four possible outliers, two with a sale price over $700,000 and two with over 4500 sq. ft. but a much lower price. If we follow the trend of the graph the higher two outliers seem to fit the pattern and we can likely keep these two in however we should probably remove the other two outliers. As this is unique to the training dataset we'll do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(np.log(df['GrLivArea']), np.log(df['SalePrice']));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Box-Cox test quickly and easily tells us if  we need to perform a transform on our data or not by telling us a lambda value:\n",
    "\n",
    "* -1. is a reciprocal\n",
    "* -.5 is a recriprocal square root\n",
    "* 0.0 is a log transformation\n",
    "* .5 is a square toot transform and\n",
    "* 1.0 is no transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "for c in df.select_dtypes(include=['int64', 'float64']).columns.values:\n",
    "    if df[c].dtypes is not object:\n",
    "        xt, maxlog = boxcox(df[c])\n",
    "        print(\"{} lambda = {:g}\".format(c, maxlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CentralAir'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join('data','train.csv'))\n",
    "process_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
